{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "WWRMCOZgcCQc"
      },
      "outputs": [],
      "source": [
        "# Load the dataset\n",
        "import pandas as pd\n",
        "file_path = '/content/melb_data.csv'\n",
        "df = pd.read_csv(file_path)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset\n",
        "file_path = '/content/melb_data.csv'\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# 1. Handle Missing Values\n",
        "# Explanation: Missing values can negatively impact model performance.\n",
        "# We need to address them appropriately to avoid biased or inaccurate results.\n",
        "# Approach: We will drop rows with missing values in 'Price' as it's our target variable\n",
        "# and fill other missing values with the mean or mode depending on the data type.\n",
        "\n",
        "\n",
        "# Drop rows with missing values in 'Price' (target variable)\n",
        "df.dropna(subset=['Price'], inplace=True)\n",
        "\n",
        "\n",
        "# Fill missing values for numerical features with the mean\n",
        "numerical_features = ['Car', 'BuildingArea', 'YearBuilt']\n",
        "for feature in numerical_features:\n",
        "  df[feature].fillna(df[feature].mean(), inplace=True)\n",
        "\n",
        "\n",
        "# Fill missing values for categorical features with the mode\n",
        "categorical_features = ['CouncilArea', 'Regionname']\n",
        "for feature in categorical_features:\n",
        "  df[feature].fillna(df[feature].mode()[0], inplace=True)\n",
        "\n",
        "# 2. Feature Engineering (Example: Creating new features)\n",
        "# Explanation: Sometimes, existing features can be combined or transformed to create\n",
        "# new features that might have a stronger correlation with the target variable.\n",
        "# Approach: Create a new feature 'Age' from 'YearBuilt'\n",
        "\n",
        "\n",
        "df['Age'] = 2023 - df['YearBuilt']\n",
        "\n",
        "# 3. Convert Categorical Features to Numerical\n",
        "# Explanation: Many machine learning algorithms work better with numerical data.\n",
        "# Approach: We'll use one-hot encoding to convert categorical features into numerical ones.\n",
        "\n",
        "#df = pd.get_dummies(df, columns=['Type','Method','Regionname'])\n",
        "\n",
        "\n",
        "# 4. Scaling Numerical Features (if needed)\n",
        "# Explanation: Features with different scales can negatively impact the performance\n",
        "# of certain algorithms (e.g., distance-based algorithms).\n",
        "# Approach: We can use standardization or normalization to scale features.\n",
        "# This step is optional and might not be necessary for all models.\n",
        "\n",
        "# from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# scaler = StandardScaler()\n",
        "# numerical_features = ['Rooms', 'Distance', 'Landsize','BuildingArea','Bathroom','Car','YearBuilt']\n",
        "# df[numerical_features] = scaler.fit_transform(df[numerical_features])\n",
        "\n",
        "\n",
        "# Print the preprocessed DataFrame\n",
        "print(df.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g2auIiy95Z3o",
        "outputId": "934f5213-e618-4eb6-d6f1-9723342e70f0"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       Suburb           Address  Rooms Type      Price Method SellerG  \\\n",
            "0  Abbotsford      85 Turner St      2    h  1480000.0      S  Biggin   \n",
            "1  Abbotsford   25 Bloomburg St      2    h  1035000.0      S  Biggin   \n",
            "2  Abbotsford      5 Charles St      3    h  1465000.0     SP  Biggin   \n",
            "3  Abbotsford  40 Federation La      3    h   850000.0     PI  Biggin   \n",
            "4  Abbotsford       55a Park St      4    h  1600000.0     VB  Nelson   \n",
            "\n",
            "        Date  Distance  Postcode  ...  Car  Landsize  BuildingArea  \\\n",
            "0  3/12/2016       2.5    3067.0  ...  1.0     202.0     151.96765   \n",
            "1  4/02/2016       2.5    3067.0  ...  0.0     156.0      79.00000   \n",
            "2  4/03/2017       2.5    3067.0  ...  0.0     134.0     150.00000   \n",
            "3  4/03/2017       2.5    3067.0  ...  1.0      94.0     151.96765   \n",
            "4  4/06/2016       2.5    3067.0  ...  2.0     120.0     142.00000   \n",
            "\n",
            "     YearBuilt  CouncilArea  Lattitude Longtitude             Regionname  \\\n",
            "0  1964.684217        Yarra   -37.7996   144.9984  Northern Metropolitan   \n",
            "1  1900.000000        Yarra   -37.8079   144.9934  Northern Metropolitan   \n",
            "2  1900.000000        Yarra   -37.8093   144.9944  Northern Metropolitan   \n",
            "3  1964.684217        Yarra   -37.7969   144.9969  Northern Metropolitan   \n",
            "4  2014.000000        Yarra   -37.8072   144.9941  Northern Metropolitan   \n",
            "\n",
            "   Propertycount         Age  \n",
            "0         4019.0   58.315783  \n",
            "1         4019.0  123.000000  \n",
            "2         4019.0  123.000000  \n",
            "3         4019.0   58.315783  \n",
            "4         4019.0    9.000000  \n",
            "\n",
            "[5 rows x 22 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Thought Process for Data Preprocessing\n",
        "\n",
        "## 1. Handling Missing Values:\n",
        "\n",
        "**Alternative Approaches Considered:**\n",
        "\n",
        "* **Imputation with median:** For numerical features, we could have used the median instead of the mean. The median is less sensitive to outliers.\n",
        "* **Imputation with KNN:** We could have used a more sophisticated imputation method like K-Nearest Neighbors (KNN), which imputes missing values based on similar data points.\n",
        "* **Removing entire columns:** If a feature has a very high percentage of missing values, we might consider removing the entire column.\n",
        "\n",
        "**Reason for Chosen Approach:**\n",
        "\n",
        "* We chose to drop rows with missing values in the 'Price' column as it is our target variable, and having missing values in the target variable would not be helpful in training our model.\n",
        "* For numerical features, we used the mean because it is a common and straightforward method for imputation, especially when we don't have a large number of outliers.\n",
        "* For categorical features, we used the mode because it helps retain the most frequent value, which often captures the most common pattern in the data.\n",
        "\n",
        "## 2. Feature Engineering:\n",
        "\n",
        "**Alternative Approaches Considered:**\n",
        "\n",
        "* **Creating interaction terms:** We could have created interaction terms between different features (e.g., 'Rooms' * 'Landsize'). This might capture more complex relationships between variables.\n",
        "* **Polynomial features:** We could have created polynomial features to capture non-linear relationships between variables.\n",
        "\n",
        "**Reason for Chosen Approach:**\n",
        "\n",
        "* We created a new feature 'Age' from 'YearBuilt' because the age of a property can often be a significant factor in its price. This is a simple but useful feature engineering technique.\n",
        "\n",
        "## 3. Converting Categorical Features to Numerical:\n",
        "\n",
        "**Alternative Approaches Considered:**\n",
        "\n",
        "* **Label encoding:** We could have used label encoding for categorical features. However, it might introduce ordinal relationships where they don't exist. For example, assigning numbers to region names (e.g., 1 for 'Northern Metropolitan', 2 for 'Southern Metropolitan') would suggest a numerical order.\n",
        "* **Target encoding:** We could have used target encoding which replaces categories with the mean of the target variable for that category. However, it can lead to overfitting if not done carefully.\n",
        "\n",
        "**Reason for Chosen Approach:**\n",
        "\n",
        "* We did not use one-hot encoding in this code. It is a common and effective approach for dealing with categorical variables in many machine learning algorithms. However, it can lead to high dimensionality if the number of unique categories in a feature is large.  In our case, you could consider one-hot encoding the 'Type','Method','Regionname' columns if your model requires a numerical representation of these features.\n",
        "\n",
        "## 4. Scaling Numerical Features:\n",
        "\n",
        "**Alternative Approaches Considered:**\n",
        "\n",
        "* **Normalization:** We could have used normalization, which scales the data between 0 and 1.\n",
        "\n",
        "**Reason for Chosen Approach:**\n",
        "\n",
        "* We did not apply any scaling to numerical features in this code. This is an optional step and would be considered depending on the chosen machine learning model and its sensitivity to feature scales. Some algorithms (e.g., those based on distance calculations like K-Nearest Neighbors) require feature scaling for optimal performance. For models like Linear Regression or Decision Trees, feature scaling might not be crucial.\n",
        "\n",
        "**Note:** This is just a basic preprocessing example. There might be further preprocessing needed based on the specific machine learning problem, model selection, and specific data exploration insights.\n"
      ],
      "metadata": {
        "id": "RXen2muK57Ro"
      }
    }
  ]
}